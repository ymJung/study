""" requirements.txt
# PDF ì²˜ë¦¬ ë° PPT ì²˜ë¦¬ ê´€ë ¨
PyMuPDF==1.25.3
python-pptx==1.0.2
# ì´ë¯¸ì§€ ì²˜ë¦¬ ë° OCR
opencv-python==4.11.0.86
numpy==1.26.4
Pillow==11.0.0
pytesseract==0.3.13

# LangChain ë° ë²¡í„° DB ê´€ë ¨ (ì‚¬ìš©í•˜ëŠ” LangChain ëª¨ë“ˆì— ë”°ë¼ ë²„ì „ ì¡°ì •)
langchain==0.3.17
langchain-community==0.3.16
langchain-openai==0.3.4
langchain-huggingface==0.1.2
faiss-cpu==1.10.0
"""
""" Dockerfile
FROM python:3.10-slim

WORKDIR /app

RUN apt-get update && \
    apt-get install -y tesseract-ocr tesseract-ocr-kor libgl1-mesa-glx libglib2.0-0 && \
    apt-get clean && rm -rf /var/lib/apt/lists/*

COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt
COPY . .
CMD ["python", "llm_rag.py"]

"""


import os
import io
import json
import fitz  # pip install PyMuPDF
from pptx import Presentation  # pip install python-pptx
import cv2
import numpy as np
from PIL import Image
import pytesseract  # sudo apt-get install tesseract-ocr, pip install pytesseract
from langchain.text_splitter import RecursiveCharacterTextSplitter 
from langchain.docstore.document import Document
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS  # pip install faiss-cpu
from langchain.vectorstores import FAISS
from langchain_openai import ChatOpenAI
from langchain.chains import RetrievalQA
import configparser
import re

# ============================================================
# ì„¤ì • íŒŒì¼ ë¡œë“œ (config.cfg íŒŒì¼ í•„ìš”: [openai] TOKEN = <API_KEY>)
# ============================================================
config = configparser.ConfigParser()
config.read('config.cfg')
openai_api_key = config['openai']['TOKEN']

# ============================================================
# ì „ì²˜ë¦¬ í•¨ìˆ˜: ì´ë¯¸ì§€ OCR ì „ì²˜ë¦¬ ë° í•œê¸€ ë„ì–´ì“°ê¸° ì •ë¦¬
# ============================================================
def preprocess_image_for_ocr_alternative(image):
    image_cv = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)
    gray = cv2.cvtColor(image_cv, cv2.COLOR_BGR2GRAY)
    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))
    equalized = clahe.apply(gray)
    sharpening_kernel = np.array([[0, -1, 0],
                                  [-1, 5, -1],
                                  [0, -1, 0]])
    sharpened = cv2.filter2D(equalized, -1, sharpening_kernel)
    processed_image = Image.fromarray(sharpened)
    return processed_image

def clean_hangul_spacing(text): # í•œê¸€ ë¬¸ì ì‚¬ì´ì— ìˆëŠ” ë¶ˆí•„ìš”í•œ ê³µë°±ì„ ì œê±°
    return re.sub(r'(?<=[ê°€-í£])\s+(?=[ê°€-í£])', '', text)

# ============================================================
# ë¬¸ì„œ ì „ì²˜ë¦¬ ë° ì²­í‚¹ í•¨ìˆ˜ (PDF PPT)
# PDF íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ(í…ìŠ¤íŠ¸ ë ˆì´ì–´ ë˜ëŠ” OCR ì ìš©)
# ë¼ì¸ë³„/ì»¬ëŸ¼ë³„ ê·¸ë£¹í™”ì™€ í›„ì²˜ë¦¬ë¥¼ í†µí•´ Document  ìƒì„±
# ë©”íƒ€ë°ì´í„°ì— í˜ì´ì§€ ë²ˆí˜¸ì™€ ì—ì´ì „íŠ¸ ì •ë³´ ë“±ì„ ê¸°ë¡
# ============================================================
def process_pdf(file_path, agent="general"):
    docs = []
    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)
    try:
        with fitz.open(file_path) as pdf:
            for page in pdf:
                text = page.get_text("text").strip()
                if not text:
                    # OCR ì ìš©: í…ìŠ¤íŠ¸ ë ˆì´ì–´ê°€ ì—†ëŠ” ê²½ìš°
                    try:
                        pix = page.get_pixmap()
                        img = Image.frombytes("RGB", (pix.width, pix.height), pix.samples)
                        processed_img = preprocess_image_for_ocr_alternative(img)
                        custom_config = r'--oem 3 --psm 6'
                        ocr_data = pytesseract.image_to_data(
                            processed_img,
                            lang="eng+kor",
                            config=custom_config,
                            output_type=pytesseract.Output.DICT
                        )
                        # 1. ê° ë¼ì¸ë³„ë¡œ ë‹¨ì–´ ì •ë³´ ìˆ˜ì§‘ (ì‹ ë¢°ë„ 60 ì´ìƒ)
                        horizontal_gap_threshold = 50  # ë‹¨ì–´ ì‚¬ì´ ê°€ë¡œ ê°„ê²© ì„ê³„ê°’
                        lines_dict = {}
                        num_boxes = len(ocr_data['text'])
                        for i in range(num_boxes):
                            try:
                                conf = float(ocr_data['conf'][i])
                            except Exception:
                                conf = 0.0
                            if conf > 60 and ocr_data['text'][i].strip():
                                line_num = ocr_data['line_num'][i]
                                if line_num not in lines_dict:
                                    lines_dict[line_num] = []
                                left = ocr_data['left'][i]
                                width = ocr_data['width'][i]
                                word = ocr_data['text'][i].strip()
                                lines_dict[line_num].append((left, width, word))
                        # 2. ê° ë¼ì¸ ë‚´ ë‹¨ì–´ë“¤ì„ ì¢Œì¸¡ ì •ë ¬ í›„, ê°€ë¡œ ê°„ê²©ì— ë”°ë¼ ê·¸ë£¹ ë¶„ë¦¬
                        grouped_lines = []
                        for line in sorted(lines_dict.keys()):
                            words = sorted(lines_dict[line], key=lambda x: x[0])
                            groups = []
                            current_group = []
                            for word_info in words:
                                if not current_group:
                                    current_group.append(word_info)
                                else:
                                    last_word = current_group[-1]
                                    gap = word_info[0] - (last_word[0] + last_word[1])
                                    if gap > horizontal_gap_threshold:
                                        groups.append(" ".join(w for _, _, w in current_group))
                                        current_group = [word_info]
                                    else:
                                        current_group.append(word_info)
                            if current_group:
                                groups.append(" ".join(w for _, _, w in current_group))
                            grouped_lines.append(groups)
                        # 3. ê° ë¼ì¸ì˜ ì»¬ëŸ¼ë³„ ê·¸ë£¹ ë³‘í•©: ê° ì»¬ëŸ¼(ì¸ë±ìŠ¤)ë³„ë¡œ ëª¨ë“  ë¼ì¸ì˜ í…ìŠ¤íŠ¸ í•©ì¹¨
                        max_cols = max(len(groups) for groups in grouped_lines)
                        merged_regions = []
                        for col in range(max_cols):
                            region_texts = []
                            for groups in grouped_lines:
                                if col < len(groups):
                                    region_texts.append(groups[col])
                            merged_regions.append(" ".join(region_texts))
                        # 4. ëª¨ë“  ì»¬ëŸ¼ì„ í•˜ë‚˜ì˜ í…ìŠ¤íŠ¸ë¡œ í•©ì¹¨ ë° í›„ì²˜ë¦¬
                        page_text = " ".join(merged_regions)
                        page_text = clean_hangul_spacing(page_text)
                        docs.append(Document(
                            page_content=page_text,
                            metadata={
                                "source": os.path.basename(file_path),
                                "page": page.number + 1,
                                "agent": agent,
                                "doc_type": "PDF"
                            }
                        ))
                    except Exception as ocr_err:
                        print(f"[{os.path.basename(file_path)}] OCR ì‹¤íŒ¨ (í˜ì´ì§€ {page.number + 1}): {ocr_err}")
                        text = ""
                else:
                    # í…ìŠ¤íŠ¸ ë ˆì´ì–´ê°€ ìˆëŠ” ê²½ìš°: ê¸°ì¡´ í…ìŠ¤íŠ¸ë¥¼ ì²­í‚¹
                    chunks = splitter.split_text(text)
                    for chunk in chunks:
                        docs.append(Document(
                            page_content=chunk,
                            metadata={
                                "source": os.path.basename(file_path),
                                "page": page.number + 1,
                                "agent": agent,
                                "doc_type": "PDF"
                            }
                        ))
    except Exception as e:
        print(f"PDF ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ ({file_path}): {e}")
    return docs

def extract_text_from_shape(shape): 
    text = ""
    if hasattr(shape, "text") and shape.text:
        text += shape.text + "\n"
    if hasattr(shape, "shapes"):
        for subshape in shape.shapes:
            text += extract_text_from_shape(subshape) # í•„ìš”í•˜ë©´ ì¬ê·€ê·€
    return text

def process_ppt(file_path, agent="general"):
    docs = []
    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)
    try:
        prs = Presentation(file_path)
        for i, slide in enumerate(prs.slides):
            slide_text = ""            
            for shape in slide.shapes:                                
                extracted = extract_text_from_shape(shape) # ì „ë¶€ì¶”ì¶œ                
                if extracted.strip():
                    slide_text += extracted + "\n"
                # ì´ë¯¸ì§€ OCR ì²˜ë¦¬
                if shape.shape_type == 13:
                    try:
                        image_blob = shape.image.blob
                        img = Image.open(io.BytesIO(image_blob))
                        processed_img = preprocess_image_for_ocr_alternative(img)
                        custom_config = r'--oem 3 --psm 6'
                        ocr_text = pytesseract.image_to_string(processed_img, lang="eng+kor", config=custom_config)
                        if ocr_text.strip():
                            slide_text += ocr_text + "\n"
                    except Exception as img_err:
                        print(f"[{os.path.basename(file_path)}] PPT ì´ë¯¸ì§€ OCR ì˜¤ë¥˜ (ìŠ¬ë¼ì´ë“œ {i+1}): {img_err}")
            if slide.has_notes_slide:
                notes_slide = slide.notes_slide
                for shape in notes_slide.shapes:
                    slide_text += extract_text_from_shape(shape) + "\n"
                    
            if slide_text.strip():
                slide_text = clean_hangul_spacing(slide_text)
                chunks = splitter.split_text(slide_text)
                for chunk in chunks:
                    docs.append(Document(
                        page_content=chunk,
                        metadata={
                            "source": os.path.basename(file_path),
                            "slide": i + 1,
                            "agent": agent,
                            "doc_type": "PPT"
                        }
                    ))
    except Exception as e:
        print(f"PPT ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ ({file_path}): {e}")
    return docs

# ============================================================
# ì„ë² ë”© ìƒì„± ë° ë²¡í„° DB êµ¬ì¶•
# ì—¬ëŸ¬ Document ë¦¬ìŠ¤íŠ¸ë¥¼, FAISS ë²¡í„° ìŠ¤í† ì–´ë¥¼ ìƒì„±
# ============================================================
def setup_vector_store(documents):
    all_docs = []
    for doc_list in documents:
        all_docs.extend(doc_list) # í•©ì³ì„œ ì²˜ë¦¬ë¦¬
    embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/paraphrase-multilingual-mpnet-base-v2") # ë‹¤êµ­ì–´ ì§€ì›ëª¨ë¸
    vectorstore = FAISS.from_documents(all_docs, embeddings)
    return vectorstore

# ============================================================
# RAG ì²´ì¸ ìƒì„± (ë²¡í„° ìŠ¤í† ì–´ + LLM ì—°ê²°)
# LLM ë‹µë³€ê³¼ í•¨ê»˜ ê´€ë ¨ ê·¼ê±°(ì¶œì²˜)ë¥¼ ë°˜í™˜
# ============================================================
def create_retrieval_qa(vectorstore, llm=None):
    if llm is None:
        llm = ChatOpenAI(model_name="gpt-4", temperature=0, openai_api_key=openai_api_key)
    qa = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=vectorstore.as_retriever(search_kwargs={"k": 3}),
        return_source_documents=True
    )
    return qa

# ============================================================
# ì¼ë°˜ ì§ˆë¬¸ ì‘ë‹µ (Retrieval ì—†ì´ ë‹¨ìˆœ LLM í™œìš©)
# ============================================================
def create_general_qa(llm=None):
    if llm is None:
        llm = ChatOpenAI(model_name="gpt-4", temperature=0, openai_api_key=openai_api_key)
    def answer(query):
        response = llm.invoke(query)
        return {"result": response, "source_documents": []}
    return answer

# ============================================================
# ì—ì´ì „íŠ¸ ë¼ìš°íŒ… ê·œì¹™ ë¡œë“œ (ì™¸ë¶€ JSON íŒŒì¼ ì§€ì›)
# ============================================================
def load_agent_routing_rules(filepath="agent_routing_rules.json"):
    """
    ì—ì´ì „íŠ¸ ë¼ìš°íŒ… ê·œì¹™ì„ ì™¸ë¶€ JSON íŒŒì¼ë¡œë¶€í„° ë¡œë“œí•©ë‹ˆë‹¤.
    íŒŒì¼ì´ ì—†ê±°ë‚˜ ì˜¤ë¥˜ê°€ ë°œìƒí•˜ë©´ ê¸°ë³¸ 
    """
    try:
        with open(filepath, "r", encoding="utf-8") as f:
            rules = json.load(f)
            return rules
    except Exception as e:
        print(f"ì—ì´ì „íŠ¸ ë¼ìš°íŒ… ê·œì¹™ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨({filepath}): {e}")
        return [
            {"agent": "deepseek", "keywords": ["ê°•í™”í•™ìŠµ", "deepseek"]},
            {"agent": "brochure", "keywords": ["ì„œë¹„ìŠ¤"]},
            {"agent": "rule", "keywords": ["íšŒì‚¬"]}
        ]

# ì „ì—­ ë³€ìˆ˜ë¡œ ë¼ìš°íŒ… ê·œì¹™ ë¡œë“œ
AGENT_ROUTING_RULES = load_agent_routing_rules()

def route_agent(query):
    """
    ì¿¼ë¦¬ ë¬¸ìì—´ì„ ë°›ì•„ì„œ, ë¯¸ë¦¬ ì •ì˜í•œ í‚¤ì›Œë“œ ê¸°ë°˜ ê·œì¹™(ì™¸ë¶€ íŒŒì¼ ë˜ëŠ” ê¸°ë³¸ê°’)ì— ë”°ë¼ ì—ì´ì „íŠ¸ë¥¼ ì„ íƒí•©ë‹ˆë‹¤.
    ìš°ì„ ìˆœìœ„ëŠ” AGENT_ROUTING_RULES ë¦¬ìŠ¤íŠ¸ì˜ ìˆœì„œë¥¼ ë”°ë¦…ë‹ˆë‹¤.
    """
    lower_query = query.lower()
    for rule in AGENT_ROUTING_RULES:
        for keyword in rule["keywords"]:
            if keyword.lower() in lower_query:
                return rule["agent"]
    return "general"

# ============================================================
# ì‚¬ìš©ì ì§ˆì˜ì‘ë‹µ ë£¨í”„ (ì—”í„° ë¬´ì‹œ, 'exit' ì…ë ¥ ì‹œ ì¢…ë£Œ)
# ============================================================
def query_loop(qa_deepseek=None, qa_brochure=None, qa_rule=None, general_qa=None):
    print("ë¬¸ì„œ ê¸°ë°˜ ì§ˆì˜ì‘ë‹µ ì‹œìŠ¤í…œì„ ì‹œì‘í•©ë‹ˆë‹¤. (ì¢…ë£Œí•˜ë ¤ë©´ 'exit' ì…ë ¥)")
    while True:
        query = input("\nğŸ”¹ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”: ").strip()
        if query == "":
            continue  # ë¹ˆ ì…ë ¥ì€ ë¬´ì‹œ
        if query.lower() == "exit":
            print("í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break

        # ì—ì´ì „íŠ¸ ì„ íƒ: ì™¸ë¶€ ê·œì¹™ì— ë”°ë¼ deepseek, brochure, rule, general ì¤‘ ì„ íƒ
        agent = route_agent(query)
        if agent == "deepseek":
            qa_chain = qa_deepseek
        elif agent == "brochure":
            qa_chain = qa_brochure
        elif agent == "rule":
            qa_chain = qa_rule
        else:
            qa_chain = None  # ì¼ë°˜ agent

        if qa_chain:
            result = qa_chain.invoke({"query": query})
            answer = result.get("result", "")
            source_docs = result.get("source_documents", [])
            if not source_docs:
                answer = "í•´ë‹¹ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                source_str = "General Agent"
            else:
                sources = set()
                for doc in source_docs:
                    meta = doc.metadata
                    if "page" in meta:
                        sources.add(f"{meta.get('source', '')} {meta.get('page', '')}í˜ì´ì§€")
                    elif "slide" in meta:
                        sources.add(f"{meta.get('source', '')} {meta.get('slide', '')}ìŠ¬ë¼ì´ë“œ")
                source_str = ", ".join(sources)
        else:
            # ì¼ë°˜ agent: retrieval ì—†ì´ LLM ë‹¨ë… ì‚¬ìš©
            res = general_qa(query)
            answer = res.get("result", "")
            source_str = "General Agent (No Document Source)"
        
        print("ë‹µë³€:", answer)
        print("ì¶œì²˜:", source_str)

# ============================================================
# main: ë¬¸ì„œ ì „ì²˜ë¦¬, ë²¡í„° ìŠ¤í† ì–´ êµ¬ì¶•, RAG ì²´ì¸ ìƒì„± ë° ì§ˆì˜ì‘ë‹µ ë£¨í”„ ì‹¤í–‰
# ============================================================
def main():
    # ë¬¸ì„œ ì²˜ë¦¬: ê° ì—ì´ì „íŠ¸ë³„ë¡œ ë©”íƒ€ë°ì´í„°ì— "agent" ê¸°ë¡
    docs_deepseek = process_pdf("pdf_file1.pdf", agent="deepseek")
    docs_brochure = process_pdf("pdf_file2.pdf", agent="brochure")
    docs_rule = process_ppt("ppt_file1.pptx", agent="rule")
    # ë²¡í„° DB êµ¬ì„±: ê° ì—ì´ì „íŠ¸ë³„ ë³„ë„ ë²¡í„° ìŠ¤í† ì–´ êµ¬ì¶•
    vectorstore_deepseek = setup_vector_store([docs_deepseek])
    vectorstore_brochure = setup_vector_store([docs_brochure])
    vectorstore_rule = setup_vector_store([docs_rule])
    
    # RAG ì²´ì¸ ìƒì„±
    qa_deepseek = create_retrieval_qa(vectorstore_deepseek)
    qa_brochure = create_retrieval_qa(vectorstore_brochure)
    qa_rule = create_retrieval_qa(vectorstore_rule)
    # ì¼ë°˜ ì—ì´ì „íŠ¸ (retrieval ë¯¸ì ìš©)
    general_qa = create_general_qa()
    
    # ì‚¬ìš©ì ì§ˆì˜ì‘ë‹µ ë£¨í”„ ì‹¤í–‰
    query_loop(qa_deepseek=qa_deepseek, qa_brochure=qa_brochure, qa_rule=qa_rule, general_qa=general_qa)


if __name__ == '__main__':
    main()

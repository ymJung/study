import os
import fitz  # pip install PyMuPDF
from pptx import Presentation  # pip install python-pptx
import cv2 # 
import numpy as np
from PIL import Image
import pytesseract # sudo apt-get install tesseract-ocr # pip install pytesseract
from langchain.text_splitter import RecursiveCharacterTextSplitter 
from langchain.docstore.document import Document

from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS  # pip install faiss-cpu

from langchain_openai import ChatOpenAI
from langchain.chains import RetrievalQA
import configparser
import re

# ============================================================
# ì„¤ì • íŒŒì¼ ë¡œë“œ (config.cfg íŒŒì¼ í•„ìš”: [openai] TOKEN = <API_KEY>)
# ============================================================
config = configparser.ConfigParser()
config.read('config.cfg')
openai_api_key = config['openai']['TOKEN']


def preprocess_image_for_ocr_alternative(image):
    # PIL ì´ë¯¸ì§€ë¥¼ OpenCV BGR ì´ë¯¸ì§€ë¡œ ë³€í™˜
    image_cv = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)    
    # ê·¸ë ˆì´ìŠ¤ì¼€ì¼ ë³€í™˜
    gray = cv2.cvtColor(image_cv, cv2.COLOR_BGR2GRAY)    
    # CLAHE ì ìš© (êµ­ë¶€ ëŒ€ë¹„ í–¥ìƒ)
    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))
    equalized = clahe.apply(gray)    
    # ìƒ¤í”„ë‹ í•„í„° ì ìš© (í…ìŠ¤íŠ¸ ê²½ê³„ ê°•í™”)
    sharpening_kernel = np.array([[0, -1, 0],
                                  [-1, 5, -1],
                                  [0, -1, 0]])
    sharpened = cv2.filter2D(equalized, -1, sharpening_kernel)    
    # ì²˜ë¦¬ëœ ì´ë¯¸ì§€ë¥¼ PIL ì´ë¯¸ì§€ë¡œ ë³€í™˜í•˜ì—¬ ë°˜í™˜
    processed_image = Image.fromarray(sharpened)
    return processed_image

def clean_hangul_spacing(text):
    """
    í•œê¸€ ë¬¸ì ì‚¬ì´ì— ìˆëŠ” ë¶ˆí•„ìš”í•œ ê³µë°±ì„ ì œê±°í•©ë‹ˆë‹¤.    
    ë‹¨, ì˜ì–´ì™€ ìˆ«ì, ê¸°í˜¸ ë“±ì€ ê·¸ëŒ€ë¡œ ë‘¡ë‹ˆë‹¤.
    """
    # í•œê¸€(ê°€-í£) ë’¤ì— ê³µë°±ì´ ìˆê³  ê·¸ ë’¤ì— í•œê¸€ì´ ì˜¤ëŠ” ê²½ìš°ë¥¼ ì°¾ì•„ì„œ ê³µë°±ì„ ì œê±°
    return re.sub(r'(?<=[ê°€-í£])\s+(?=[ê°€-í£])', '', text)


# ============================================================
# PDF ì²˜ë¦¬: í…ìŠ¤íŠ¸ ì¶”ì¶œ, OCR (í…ìŠ¤íŠ¸ ì—†ì„ ê²½ìš°), ì²­í‚¹ ë° ë©”íƒ€ë°ì´í„° ì¶”ê°€
# agent: deepseek, brochure, ë˜ëŠ” general (ê¸°ë³¸ê°’)
# ============================================================
def process_pdf(file_path, agent="general"):
    docs = []
    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)
    
    try:
        with fitz.open(file_path) as pdf:
            for page in pdf:
                text = page.get_text("text").strip()
                # í…ìŠ¤íŠ¸ê°€ ì—†ëŠ” ê²½ìš° OCR ì ìš©
                if not text:
                    try:
                        pix = page.get_pixmap()
                        img = Image.frombytes("RGB", (pix.width, pix.height), pix.samples)
                        processed_img = preprocess_image_for_ocr_alternative(img)
                        custom_config = r'--oem 3 --psm 6'
                        
                        # ë‹¨ì–´ë³„ OCR ê²°ê³¼(ì¢Œí‘œ, í…ìŠ¤íŠ¸, ì‹ ë¢°ë„ ë“±)ë¥¼ ì¶”ì¶œ
                        ocr_data = pytesseract.image_to_data(
                            processed_img,
                            lang="eng+kor",
                            config=custom_config,
                            output_type=pytesseract.Output.DICT
                        )
                        
                        # 1. ê° ë¼ì¸(line_num) ë³„ë¡œ ë‹¨ì–´ ì •ë³´ë¥¼ ìˆ˜ì§‘ (ì‹ ë¢°ë„ 60 ì´ìƒ)
                        horizontal_gap_threshold = 50  # ë‹¨ì–´ ì‚¬ì´ ê°€ë¡œ ê°„ê²© ì„ê³„ê°’ (í”½ì…€)
                        lines_dict = {}  # key: line_num, value: list of (left, width, word)
                        num_boxes = len(ocr_data['text'])
                        for i in range(num_boxes):
                            try:
                                conf = float(ocr_data['conf'][i])
                            except Exception:
                                conf = 0.0
                            if conf > 60 and ocr_data['text'][i].strip():
                                line_num = ocr_data['line_num'][i]
                                if line_num not in lines_dict:
                                    lines_dict[line_num] = []
                                left = ocr_data['left'][i]
                                width = ocr_data['width'][i]
                                word = ocr_data['text'][i].strip()
                                lines_dict[line_num].append((left, width, word))
                        
                        # 2. ê° ë¼ì¸ ë‚´ì—ì„œ ë‹¨ì–´ë“¤ì„ ì¢Œì¸¡ ì¢Œí‘œ ìˆœìœ¼ë¡œ ì •ë ¬í•œ í›„,
                        #    ê°€ë¡œ ê°„ê²©(gap)ì´ horizontal_gap_threshold ì´ìƒì´ë©´ ë‹¤ë¥¸ ê·¸ë£¹ìœ¼ë¡œ ë¶„ë¦¬
                        grouped_lines = []
                        for line in sorted(lines_dict.keys()):
                            words = sorted(lines_dict[line], key=lambda x: x[0])
                            groups = []
                            current_group = []
                            for word_info in words:
                                if not current_group:
                                    current_group.append(word_info)
                                else:
                                    last_word = current_group[-1]
                                    gap = word_info[0] - (last_word[0] + last_word[1])
                                    if gap > horizontal_gap_threshold:
                                        groups.append(" ".join(w for _, _, w in current_group))
                                        current_group = [word_info]
                                    else:
                                        current_group.append(word_info)
                            if current_group:
                                groups.append(" ".join(w for _, _, w in current_group))
                            grouped_lines.append(groups)
                        
                        # 3. ë¼ì¸ë³„ ê·¸ë£¹ì´ ëª¨ë‘ ë¦¬ìŠ¤íŠ¸(ì˜ˆ: ê° ë¼ì¸ì—ì„œ ì»¬ëŸ¼ë³„ë¡œ ë¶„ë¦¬ë¨)
                        #    ê° ë¼ì¸ì˜ ê·¸ë£¹ ìˆ˜ëŠ” ë‹¤ë¥¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ, ë¨¼ì € ìµœëŒ€ ê·¸ë£¹ ê°œìˆ˜ë¥¼ íŒŒì•…
                        max_cols = max(len(groups) for groups in grouped_lines)
                        
                        # 4. ê°™ì€ ì¸ë±ìŠ¤(ì»¬ëŸ¼) ê·¸ë£¹ë¼ë¦¬ í•©ì³ì„œ í•˜ë‚˜ì˜ êµ¬ì—­(region) í…ìŠ¤íŠ¸ë¡œ ë§Œë“¦
                        merged_regions = []  # ê° ìš”ì†Œê°€ í•˜ë‚˜ì˜ ì»¬ëŸ¼ì— í•´ë‹¹í•˜ëŠ” í…ìŠ¤íŠ¸
                        for col in range(max_cols):
                            region_texts = []
                            for groups in grouped_lines:
                                if col < len(groups):
                                    region_texts.append(groups[col])
                            merged_regions.append(" ".join(region_texts))
                        
                        # 5. ëª¨ë“  merged region(ì»¬ëŸ¼)ì„ í•˜ë‚˜ì˜ í…ìŠ¤íŠ¸ë¡œ í•©ì³ì„œ Document ìƒì„±
                        page_text = " ".join(merged_regions)                        
                        page_text = clean_hangul_spacing(page_text)                        
                        docs.append(Document(
                            page_content=page_text,
                            metadata={
                                "source": os.path.basename(file_path),
                                "page": page.number + 1,
                                "agent": agent,
                                "doc_type": "PDF"
                            }
                        ))
                        
                    except Exception as ocr_err:
                        print(f"[{os.path.basename(file_path)}] OCR ì‹¤íŒ¨ (í˜ì´ì§€ {page.number + 1}): {ocr_err}")
                        text = ""
                else:
                    # í…ìŠ¤íŠ¸ ë ˆì´ì–´ê°€ ìˆëŠ” ê²½ìš°
                    chunks = splitter.split_text(text)
                    for chunk in chunks:
                        docs.append(Document(
                            page_content=chunk,
                            metadata={
                                "source": os.path.basename(file_path),
                                "page": page.number + 1,
                                "agent": agent,
                                "doc_type": "PDF"
                            }
                        ))
    except Exception as e:
        print(f"PDF ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ ({file_path}): {e}")
    return docs


# ============================================================
# PPT ì²˜ë¦¬: ìŠ¬ë¼ì´ë“œ í…ìŠ¤íŠ¸, ë…¸íŠ¸ ì¶”ì¶œ, (ì´ë¯¸ì§€ OCR ìë¦¬) ë° ë©”íƒ€ë°ì´í„° ì¶”ê°€
# ============================================================
def process_ppt(file_path, agent="general"):
    docs = []
    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)
    
    try:
        prs = Presentation(file_path)
        for i, slide in enumerate(prs.slides):
            slide_text = ""
            # ìŠ¬ë¼ì´ë“œ ë‚´ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            for shape in slide.shapes:
                if hasattr(shape, "text") and shape.text:
                    slide_text += shape.text + "\n"
                # ì´ë¯¸ì§€ OCR ì²˜ë¦¬ (ì˜ˆì‹œ)
                if shape.shape_type == 13:  # MSO_SHAPE_TYPE.PICTURE (ìƒìˆ˜ê°’ 13)
                    try:
                        # ì‹¤ì œ ì´ë¯¸ì§€ ë°ì´í„° ì¶”ì¶œ ë° OCR ì²˜ë¦¬ëŠ” PPTX ë‚´ë¶€ êµ¬ì¡°ì— ë”°ë¼ ë‹¤ë¦„.
                        # í•„ìš”ì‹œ shape.image ë˜ëŠ” ê´€ë ¨ APIë¥¼ í™œìš©í•´ ì´ë¯¸ì§€ ë°ì´í„°ë¥¼ ì¶”ì¶œ í›„ OCR ì ìš©
                        pass
                    except Exception as img_err:
                        print(f"[{os.path.basename(file_path)}] PPT ì´ë¯¸ì§€ OCR ì˜¤ë¥˜ (ìŠ¬ë¼ì´ë“œ {i+1}): {img_err}")
            # ìŠ¬ë¼ì´ë“œ ë…¸íŠ¸ ì¶”ì¶œ
            if slide.has_notes_slide:
                notes_slide = slide.notes_slide
                for shape in notes_slide.shapes:
                    if hasattr(shape, "text") and shape.text:
                        slide_text += shape.text + "\n"
            if slide_text.strip():                
                chunks = splitter.split_text(slide_text)
                for chunk in chunks:
                    metadata = {
                        "source": os.path.basename(file_path),
                        "slide": i + 1,
                        "agent": agent,
                        "doc_type": "PPT"
                    }
                    docs.append(Document(page_content=chunk, metadata=metadata))
    except Exception as e:
        print(f"PPT ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ ({file_path}): {e}")
    return docs

# ============================================================
# ì—¬ëŸ¬ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ë¥¼ í•©ì³ FAISS ë²¡í„° ìŠ¤í† ì–´ ìƒì„±
# ============================================================
def setup_vector_store(documents):
    all_docs = []
    for doc_list in documents:
        all_docs.extend(doc_list)
    # í•œêµ­ì–´ ì§€ì›ì´ ë” ì¢‹ì€ ë‹¤êµ­ì–´ ëª¨ë¸ë¡œ ë³€ê²½
    embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/paraphrase-multilingual-mpnet-base-v2")
    vectorstore = FAISS.from_documents(all_docs, embeddings)
    return vectorstore

# ============================================================
# RAG ì²´ì¸ ìƒì„± (ë²¡í„° ìŠ¤í† ì–´ + LLM ì—°ê²°)
# ============================================================
def create_retrieval_qa(vectorstore, llm=None):
    if llm is None:
        llm = ChatOpenAI(model_name="gpt-4", temperature=0, openai_api_key=openai_api_key)
    qa = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=vectorstore.as_retriever(search_kwargs={"k": 3}),
        return_source_documents=True
    )
    return qa

# ============================================================
# ì¼ë°˜ ì§ˆë¬¸ ì‘ë‹µ (retrieval ì—†ì´ ë‹¨ìˆœ LLM í™œìš©)
# ============================================================
def create_general_qa(llm=None):
    if llm is None:
        llm = ChatOpenAI(model_name="gpt-4", temperature=0, openai_api_key=openai_api_key)
    def answer(query):        
        response = llm.invoke({"query": query})
        return {"result": response, "source_documents": []}
    return answer


AGENT_ROUTING_RULES = [
    {
        "agent": "deepseek",
        "keywords": ["ê°•í™”í•™ìŠµ", "deepseek"]
    },
    {
        "agent": "brochure",
        "keywords": ["ì„œë¹„ìŠ¤"]
    },
    {
        "agent": "rule",
        "keywords": ["ë‚¨ë…€ê³ ìš©í‰ë“±ë²•", "íšŒì‚¬"]
    }
]
# ============================================================
# ì§ˆë¬¸ ë‚´ìš©ì— ë”°ë¥¸ ì—ì´ì „íŠ¸ ë¼ìš°íŒ… (ê°„ë‹¨í•œ í‚¤ì›Œë“œ ê¸°ë°˜ ì˜ˆì‹œ)
# ============================================================
def route_agent(query):
    """
    ì¿¼ë¦¬ ë¬¸ìì—´ì„ ë°›ì•„ì„œ, ë¯¸ë¦¬ ì •ì˜í•œ í‚¤ì›Œë“œ ê¸°ë°˜ ê·œì¹™ì— ë”°ë¼ ì—ì´ì „íŠ¸ë¥¼ ì„ íƒí•©ë‹ˆë‹¤.
    í•´ë‹¹í•˜ëŠ” í‚¤ì›Œë“œê°€ ì—¬ëŸ¬ ê°œ ì¼ ê²½ìš°, ìš°ì„ ìˆœìœ„ëŠ” AGENT_ROUTING_RULES ë¦¬ìŠ¤íŠ¸ì˜ ìˆœì„œë¥¼ ë”°ë¦…ë‹ˆë‹¤.
    """
    lower_query = query.lower()
    for rule in AGENT_ROUTING_RULES:
        for keyword in rule["keywords"]:
            if keyword.lower() in lower_query:
                return rule["agent"]
    return "general"

# ============================================================
# ì‚¬ìš©ì ì§ˆì˜ì‘ë‹µ ë£¨í”„: ì§ˆë¬¸ì— ë”°ë¼ ì ì ˆí•œ ì—ì´ì „íŠ¸(RAG ì²´ì¸ ë˜ëŠ” ì¼ë°˜ LLM) ì„ íƒ
# ============================================================
def query_loop(qa_deepseek=None, qa_brochure=None, qa_rule=None, general_qa=None):
    print("ë¬¸ì„œ ê¸°ë°˜ ì§ˆì˜ì‘ë‹µ ì‹œìŠ¤í…œì„ ì‹œì‘í•©ë‹ˆë‹¤. (ì¢…ë£Œí•˜ë ¤ë©´ 'exit' ì…ë ¥)")
    while True:
        query = input("\nğŸ”¹ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”: ")
        if query.lower().strip() == "exit":
            print("í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break

        # ì—ì´ì „íŠ¸ ì„ íƒ (deepseek / brochure / general)
        agent = route_agent(query)
        if agent == "deepseek":
            qa_chain = qa_deepseek
        elif agent == "brochure":
            qa_chain = qa_brochure
        elif agent == "rule":
            qa_chain = qa_rule
        else:
            qa_chain = None  # general agent

        if qa_chain:
            result = qa_chain.invoke({"query": query})
            answer = result.get("result", "")
            source_docs = result.get("source_documents", [])
            
            # ê´€ë ¨ ë¬¸ì„œê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ ë©”ì‹œì§€ ì¶œë ¥
            if not source_docs:
                answer = "í•´ë‹¹ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                source_str = "General Agent"
            else:
                sources = set()
                for doc in source_docs:
                    meta = doc.metadata
                    if "page" in meta:
                        sources.add(f"{meta.get('source', '')} {meta.get('page', '')}í˜ì´ì§€")
                    elif "slide" in meta:
                        sources.add(f"{meta.get('source', '')} {meta.get('slide', '')}ìŠ¬ë¼ì´ë“œ")
                source_str = ", ".join(sources)
        else:
            # general agent: retrieval ì—†ì´ LLM ë‹¨ë… ì‚¬ìš©
            res = general_qa(query)
            answer = res.get("result", "")
            source_str = "General Agent (No Document Source)"
        
        print("ë‹µë³€:", answer)
        print("ì¶œì²˜:", source_str)

# ============================================================
# main: ê° ë¬¸ì„œë³„ ì—ì´ì „íŠ¸ ì§€ì •, ë²¡í„° ìŠ¤í† ì–´ ë° ì²´ì¸ êµ¬ì„± í›„ ì§ˆì˜ì‘ë‹µ ë£¨í”„ ì‹¤í–‰
# ============================================================
def main():
    # ë¬¸ì„œ ì²˜ë¦¬: ì—ì´ì „íŠ¸ë³„ë¡œ ë©”íƒ€ë°ì´í„°ì— "agent"ë¥¼ ê¸°ë¡
    docs_deepseek = process_pdf("pdf_file1.pdf", agent="deepseek")
    docs_brochure = process_pdf("pdf_file2.pdf", agent="brochure")
    docs_rule = process_ppt("ppt_file1.pptx", agent="rule")
    # ë²¡í„° DB êµ¬ì„±: ê° ì—ì´ì „íŠ¸ë³„ ë³„ë„ ë²¡í„° ìŠ¤í† ì–´ êµ¬ì¶•
    vectorstore_deepseek = setup_vector_store([docs_deepseek])
    vectorstore_brochure = setup_vector_store([docs_brochure])
    vectorstore_rule = setup_vector_store([docs_rule])
    
    # RAG ì²´ì¸ ìƒì„±
    qa_deepseek = create_retrieval_qa(vectorstore_deepseek)
    qa_brochure = create_retrieval_qa(vectorstore_brochure)
    qa_rule = create_retrieval_qa(vectorstore_rule)
    # ì¼ë°˜ ì—ì´ì „íŠ¸ (retrieval ë¯¸ì ìš©)
    general_qa = create_general_qa()
    
    # ì‚¬ìš©ì ì§ˆì˜ì‘ë‹µ ë£¨í”„ ì‹¤í–‰
    query_loop(qa_deepseek, qa_brochure, qa_rule, general_qa)
    
    
if __name__ == '__main__':
    main() 